{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "799edae8",
   "metadata": {},
   "source": [
    "## Criando Dados Fakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c77be6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/17 15:10:55 WARN Utils: Your hostname, maccpcn.local resolves to a loopback address: 127.0.0.1; using 192.168.15.106 instead (on interface en0)\n",
      "25/06/17 15:10:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/17 15:10:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/06/17 15:10:56 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import Row, functions as f, DataFrame as SparkDataFrame\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "import random, os\n",
    "# Spark connection\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"local\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"1\")\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "def generate_cpf_from_int(seed: int) -> str:\n",
    "    rng = random.Random(seed)\n",
    "    n = [rng.randint(0, 9) for _ in range(9)]\n",
    "    s = sum([(10 - i) * n[i] for i in range(9)])\n",
    "    d1 = 11 - s % 11\n",
    "    if d1 >= 10:\n",
    "        d1 = 0\n",
    "    n.append(d1)\n",
    "    s = sum([(11 - i) * n[i] for i in range(10)])\n",
    "    d2 = 11 - s % 11\n",
    "    if d2 >= 10:\n",
    "        d2 = 0\n",
    "    n.append(d2)\n",
    "    return ''.join(map(str, n))\n",
    "\n",
    "def generate_cnpj_from_int(seed: int) -> str:\n",
    "    rng = random.Random(seed)\n",
    "    n = [rng.randint(0, 9) for _ in range(12)]\n",
    "    s = sum([n[i] * x for i, x in enumerate([5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 2])])\n",
    "    d1 = 11 - s % 11\n",
    "    if d1 >= 10:\n",
    "        d1 = 0\n",
    "    n.append(d1)\n",
    "    s = sum([n[i] * x for i, x in enumerate([6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 2])])\n",
    "    d2 = 11 - s % 11\n",
    "    if d2 >= 10:\n",
    "        d2 = 0\n",
    "    n.append(d2)\n",
    "    return ''.join(map(str, n))\n",
    "\n",
    "def make_personas_table(qtd_f: int, qtd_j: int) -> SparkDataFrame:\n",
    "\n",
    "    rng = random.Random(42)\n",
    "\n",
    "    banks = [1,2,3,4,5]\n",
    "\n",
    "    data = []\n",
    "    dict_uniques = {}\n",
    "\n",
    "    # Pessoa Física (CPF)\n",
    "    for i in range(qtd_f):\n",
    "        cpf = generate_cpf_from_int(i)\n",
    "\n",
    "        p = rng.random()\n",
    "        if p < 0.5:\n",
    "            num_accounts = 1\n",
    "        elif p < 0.95:\n",
    "            num_accounts = 2\n",
    "        else:\n",
    "            # Distribuição exponencial decrescente para 3 a 10\n",
    "            num_accounts = min(10, int(rng.expovariate(0.8)) + 3)\n",
    "        \n",
    "        banks_for_person = []\n",
    "\n",
    "        if rng.random() < 0.8:\n",
    "            banks_for_person.append(1)\n",
    "        \n",
    "        while len(banks_for_person) < num_accounts:\n",
    "            b = rng.choice(banks)\n",
    "            if rng.random() < 0.05:\n",
    "                # 5% de chance de ter mais de uma conta no mesmo banco\n",
    "                banks_for_person.append(b)\n",
    "            else:\n",
    "                if b not in banks_for_person:\n",
    "                    banks_for_person.append(b)\n",
    "\n",
    "        for b in banks_for_person:\n",
    "            \n",
    "            while True:\n",
    "                nr_office = str(rng.randint(1000, 4999))\n",
    "                nr_account = str(rng.randint(100000, 499999))\n",
    "\n",
    "                fl = dict_uniques.get((nr_office,nr_account,b), False)\n",
    "                \n",
    "                if fl:\n",
    "                    continue\n",
    "                else:\n",
    "                    dict_uniques[(nr_office,nr_account,b)] = True\n",
    "                    break\n",
    "\n",
    "            data.append((\n",
    "                cpf, 'F', str(b), nr_office, nr_account\n",
    "            ))\n",
    "\n",
    "    ### Pessoa Jurídica (CNPJ)\n",
    "    for i in range(qtd_j):\n",
    "        cnpj = generate_cnpj_from_int(i)\n",
    "\n",
    "        num_accounts = rng.randint(1, 10)\n",
    "\n",
    "        for _ in range(num_accounts):\n",
    "\n",
    "            b = rng.choice(banks)\n",
    "\n",
    "            while True:\n",
    "\n",
    "                nr_office = str(rng.randint(1000, 4999))\n",
    "                nr_account = str(rng.randint(100000, 499999))\n",
    "\n",
    "                fl = dict_uniques.get((nr_office,nr_account,b), False)\n",
    "                \n",
    "                if fl:\n",
    "                    continue\n",
    "                else:\n",
    "                    dict_uniques[(nr_office,nr_account,b)] = True\n",
    "                    break\n",
    "\n",
    "            data.append((\n",
    "                cnpj, 'J', str(b), nr_office, nr_account\n",
    "            ))\n",
    "        \n",
    "    schema = StructType([\n",
    "        StructField(\"nr_cpf_cnpj\", StringType(), False),\n",
    "        StructField(\"cd_tipo_doc\", StringType(), True),\n",
    "        StructField(\"cd_bank\", StringType(), True),\n",
    "        StructField(\"nr_office\", StringType(), True),\n",
    "        StructField(\"nr_account\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "    return spark.createDataFrame(data, schema=schema)\n",
    "   \n",
    "def make_transactional_tables(personas_df, start_date: str, end_date: str):\n",
    "\n",
    "    rng = random.Random(123)\n",
    "\n",
    "    contas_list = [{\n",
    "        \"nr_cpf_cnpj\": row[\"nr_cpf_cnpj\"],\n",
    "        \"cd_tipo_doc\": row[\"cd_tipo_doc\"],\n",
    "        \"cd_bank\": row[\"cd_bank\"],\n",
    "        \"nr_office\": row[\"nr_office\"],\n",
    "        \"nr_account\": row[\"nr_account\"]\n",
    "    } for row in personas_df.collect()]\n",
    "\n",
    "    cpfs = [c for c in contas_list if c[\"cd_tipo_doc\"] == 'F']\n",
    "    cnpjs = [c for c in contas_list if c[\"cd_tipo_doc\"] == 'J']\n",
    "\n",
    "    data = []\n",
    "    start_dt = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end_dt = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    days_range = (end_dt - start_dt).days + 1\n",
    "\n",
    "    for day in range(days_range):\n",
    "        current_date = start_dt + timedelta(days=day)\n",
    "        pct = rng.uniform(0.1, 0.7)\n",
    "        num_participantes = int(pct * len(cpfs))\n",
    "        participantes = rng.sample(cpfs, num_participantes)\n",
    "        total_ted_doc = rng.randint(num_participantes // 4, num_participantes // 2)\n",
    "        total_pix = total_ted_doc * 2\n",
    "        transacoes = total_pix + total_ted_doc\n",
    "        for _ in range(transacoes):\n",
    "            sender = rng.choice(participantes)\n",
    "            \n",
    "            if rng.random() < 0.6 and len(cnpjs) > 0:\n",
    "                receiver = rng.choice(cnpjs)\n",
    "            else:\n",
    "                receiver = rng.choice(cpfs)\n",
    "            \n",
    "            if receiver == sender:\n",
    "                receiver = rng.choice(cpfs + cnpjs)\n",
    "            \n",
    "            if rng.random() < total_pix / (total_pix + total_ted_doc):\n",
    "                tp_transacao = \"PIX\"\n",
    "            else:\n",
    "                tp_transacao = rng.choice([\"TED\", \"DOC\"])\n",
    "\n",
    "            valor = round(min(rng.expovariate(1/300), 10000), 2)\n",
    "\n",
    "            data.append((\n",
    "                sender[\"nr_cpf_cnpj\"],\n",
    "                sender[\"nr_account\"],\n",
    "                sender[\"nr_office\"],\n",
    "                sender[\"cd_bank\"],\n",
    "\n",
    "                receiver[\"nr_cpf_cnpj\"],\n",
    "                receiver[\"nr_account\"],\n",
    "                receiver[\"nr_office\"],\n",
    "                receiver[\"cd_bank\"],\n",
    "\n",
    "                valor,\n",
    "                current_date.strftime('%Y-%m-%d'),\n",
    "                tp_transacao\n",
    "            ))\n",
    "\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField(\"nr_cpf_cnpj_sender\", StringType(), True),\n",
    "        StructField(\"nr_account_sender\", StringType(), True),\n",
    "        StructField(\"nr_office_sender\", StringType(), True),\n",
    "        StructField(\"cd_bank_sender\", StringType(), True),\n",
    "\n",
    "        StructField(\"nr_cpf_cnpj_receiver\", StringType(), True),\n",
    "        StructField(\"nr_account_receiver\", StringType(), True),\n",
    "        StructField(\"nr_office_receiver\", StringType(), True),\n",
    "        StructField(\"cd_bank_receiver\", StringType(), True),\n",
    "\n",
    "        StructField(\"vl_pgto\", DoubleType(), True),\n",
    "        StructField(\"dat_ref_carga\", StringType(), True),\n",
    "        StructField(\"tp_transacao\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "    df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "    df_pix = df.filter(f.col(\"tp_transacao\") == \"PIX\").select(df.columns[:-1])\n",
    "    df_ted = df.filter(f.col(\"tp_transacao\") == \"TED\").select(df.columns[:-1])\n",
    "    df_doc = df.filter(f.col(\"tp_transacao\") == \"DOC\").select(df.columns[:-1])\n",
    "\n",
    "    return df_pix, df_ted, df_doc\n",
    "\n",
    "def make_campanha_table(personas_df, end_date: str):\n",
    "    nr_cpf_list = personas_df.filter(f.col('cd_tipo_doc') == 'F').collect()\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for row in nr_cpf_list:\n",
    "        data.append((\n",
    "            row['nr_cpf_cnpj'],\n",
    "            0,\n",
    "            0,\n",
    "            end_date\n",
    "        ))\n",
    "    \n",
    "    schema = StructType([\n",
    "            StructField(\"nr_cpf_cnpj\", StringType(), True),\n",
    "            StructField(\"camp_disparo\", IntegerType(), True),\n",
    "            StructField(\"camp_conversao\", IntegerType(), True),\n",
    "            StructField(\"dat_ref_carga\", StringType(), True)\n",
    "        ])\n",
    "\n",
    "    return spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "def make_inadimplencia_table(personas_df, transacoes_df):\n",
    "\n",
    "    rng = random.Random(42)\n",
    "\n",
    "    cnpjs_receb = transacoes_df.join(\n",
    "        personas_df.select('nr_cpf_cnpj', 'cd_tipo_doc').withColumnRenamed(\"nr_cpf_cnpj\", \"nr_cpf_cnpj_receiver\"),\n",
    "        on=[\"nr_cpf_cnpj_receiver\"],\n",
    "        how=\"inner\"\n",
    "    ).filter(f.col(\"cd_tipo_doc\") == \"J\")\n",
    "\n",
    "\n",
    "    cnpj_agg = cnpjs_receb.select('nr_cpf_cnpj_receiver', 'vl_pgto').groupBy(\"nr_cpf_cnpj_receiver\").agg(\n",
    "        f.sum(\"vl_pgto\").alias(\"vl_total_recebido\")\n",
    "    )\n",
    "\n",
    "    window_cnpj = Window.orderBy(f.col(\"vl_total_recebido\").desc())\n",
    "\n",
    "    cnpj_ranked = cnpj_agg.withColumn(\n",
    "        \"rank\", f.row_number().over(window_cnpj)\n",
    "    )\n",
    "\n",
    "    cnpj_cartola_df = cnpj_ranked.filter(\n",
    "        (f.col(\"rank\") >= 875) & (f.col(\"rank\") < 900)\n",
    "    ).select(\n",
    "        f.col(\"nr_cpf_cnpj_receiver\").alias(\"nr_cpf_cnpj\"),\n",
    "        f.lit(1).alias(\"fl_cartola\")\n",
    "    )\n",
    "\n",
    "    trans_to_cartola = transacoes_df.join(\n",
    "        cnpj_cartola_df.select(f.col('nr_cpf_cnpj').alias('nr_cpf_cnpj_receiver')),\n",
    "        on=['nr_cpf_cnpj_receiver'],\n",
    "        how=\"inner\"\n",
    "    )\n",
    "\n",
    "    last_date_trans = trans_to_cartola.groupBy('nr_cpf_cnpj_sender').agg(\n",
    "        f.max('dat_ref_carga').alias('data_ulti_trans_cartola')\n",
    "    ).distinct()\n",
    "\n",
    "    last_date_trans = last_date_trans.withColumnRenamed('nr_cpf_cnpj_sender', 'nr_cpf_cnpj')\n",
    "\n",
    "    riscos_df = personas_df.filter(f.col('cd_tipo_doc') == 'F').select('nr_cpf_cnpj').distinct().join(\n",
    "        last_date_trans,\n",
    "        on = ['nr_cpf_cnpj'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    cpfs = personas_df.filter(f.col(\"cd_tipo_doc\") == \"F\").select(\"nr_cpf_cnpj\").collect()\n",
    "\n",
    "    num_random_inad = max(1, int(len(cpfs) * 0.005))\n",
    "\n",
    "    random_inads = rng.sample(cpfs, num_random_inad)\n",
    "\n",
    "    dates = [row[\"dat_ref_carga\"] for row in transacoes_df.select(\"dat_ref_carga\").distinct().collect()]\n",
    "\n",
    "    random_dates_dict = {row.nr_cpf_cnpj: rng.choice(dates) for row in random_inads}\n",
    "\n",
    "    random_inads_df = spark.createDataFrame(\n",
    "        [Row(nr_cpf_cnpj=k, data_ulti_trans_cartola_1=v) for k, v in random_dates_dict.items()]\n",
    "    )\n",
    "\n",
    "    riscos_df = riscos_df.join(\n",
    "        random_inads_df, on=\"nr_cpf_cnpj\", how=\"left\"\n",
    "    ).withColumn(\n",
    "        \"data_ulti_trans_cartola\",\n",
    "        f.coalesce(\n",
    "            f.col(\"data_ulti_trans_cartola\"), \n",
    "            f.col(\"data_ulti_trans_cartola_1\")\n",
    "        )\n",
    "    ).drop(\"data_ulti_trans_cartola_1\")\n",
    "\n",
    "    riscos_df = riscos_df.filter(f.col(\"data_ulti_trans_cartola\").isNotNull()).select(\n",
    "        f.col('nr_cpf_cnpj'),\n",
    "        f.col('data_ulti_trans_cartola').alias('dat_ref_carga'),\n",
    "        f.lit(1).alias(\"flag_bloqueio\")\n",
    "    )\n",
    "\n",
    "    return cnpj_cartola_df, riscos_df\n",
    "\n",
    "\n",
    "# start_date = \"2024-01-01\"\n",
    "# end_date = \"2024-06-30\"\n",
    "\n",
    "# personas_path = \"../data/02_intermediate/personas\"\n",
    "# if os.path.exists(personas_path):\n",
    "#     df_personas = spark.read.parquet(personas_path)\n",
    "# else:\n",
    "#     df_personas = make_personas_table(qtd_f=50000, qtd_j=1000)\n",
    "#     df_personas.write.mode(\"overwrite\").parquet(personas_path)\n",
    "\n",
    "\n",
    "# path_pix = \"../data/01_raw/raw_pix\"\n",
    "# path_ted = \"../data/01_raw/raw_ted\"\n",
    "# path_doc = \"../data/01_raw/raw_doc\"\n",
    "# if os.path.exists(path_pix):\n",
    "#     df_pix = spark.read.parquet(path_pix)\n",
    "#     df_ted = spark.read.parquet(path_ted)\n",
    "#     df_doc = spark.read.parquet(path_doc)\n",
    "# else:\n",
    "#     df_pix, df_ted, df_doc = make_transactional_tables(df_personas, start_date, end_date)\n",
    "#     df_pix.write.mode(\"overwrite\").parquet(path_pix)\n",
    "#     df_ted.write.mode(\"overwrite\").parquet(path_ted)\n",
    "#     df_doc.write.mode(\"overwrite\").parquet(path_doc)\n",
    "\n",
    "\n",
    "# path_campanhas = \"../data/02_intermediate/campanhas\"\n",
    "\n",
    "# if os.path.exists(path_campanhas):\n",
    "#     df_campanhas = spark.read.parquet(path_campanhas)\n",
    "# else:\n",
    "#     df_campanhas = make_campanha_table(df_personas, end_date=end_date)\n",
    "#     df_campanhas.write.mode(\"overwrite\").parquet(path_campanhas)\n",
    "\n",
    "# path_inadimplentes = \"../data/02_intermediate/riscos\"\n",
    "\n",
    "# path_cartola = \"../data/02_intermediate/cartola\"\n",
    "# path_riscos = \"../data/02_intermediate/riscos\"\n",
    "\n",
    "# if os.path.exists(path_cartola):\n",
    "#     df_cartola = spark.read.parquet(path_cartola)\n",
    "#     df_riscos = spark.read.parquet(path_riscos)\n",
    "# else:\n",
    "#     df_cartola, df_riscos = make_inadimplencia_table(df_personas, df_pix.union(df_ted).union(df_doc))\n",
    "#     df_cartola.write.mode(\"overwrite\").parquet(path_cartola)\n",
    "#     df_riscos.write.mode(\"overwrite\").parquet(path_riscos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fd5c1a",
   "metadata": {},
   "source": [
    "## Validação de balanceamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "732f24b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|flag_bloqueio|count|\n",
      "+-------------+-----+\n",
      "|            1|24993|\n",
      "|            0|25006|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "master_table = spark.read.parquet(\"../data/04_model_input/master_table\")\n",
    "master_table.groupBy(f.col(\"flag_bloqueio\")).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859a00eb",
   "metadata": {},
   "source": [
    "## Resultado Scoragem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff154e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_table = spark.read.parquet(\"../data/05_score/score_table\")\n",
    "master_table.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
